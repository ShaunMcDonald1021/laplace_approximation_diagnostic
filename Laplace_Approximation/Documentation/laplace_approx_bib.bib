@mastersthesis{Zhou2017,
author = {Zhou, Haoxuan},
file = {:C$\backslash$:/Users/Shaun.DESKTOP-85KMIQG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou - 2017 - Bayesian Integration for Assessing the Quality of the Laplace Approximation.pdf:pdf},
month = {nov},
school = {Simon Fraser University},
title = {{Bayesian Integration for Assessing the Quality of the Laplace Approximation}},
url = {http://summit.sfu.ca/item/17765},
year = {2017}
}

@article{Chkrebtii2016,
abstract = {We explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. Within the calibration problem, dis-cretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics.},
author = {Chkrebtii, Oksana A and Campbell, David A and Calderhead, Ben and Girolami, Mark A},
doi = {10.1214/16-BA1036},
file = {:C$\backslash$:/Users/Shaun.DESKTOP-85KMIQG/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chkrebtii et al. - 2016 - Bayesian Solution Uncertainty Quantification for Differential Equations.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Bayesian numerical analysis,Gaussian processes,differential equation models,uncertainty in computer models,uncertainty quantification},
number = {4},
pages = {1239--1267},
title = {{Bayesian Solution Uncertainty Quantification for Differential Equations}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ba/1473276259},
volume = {11},
year = {2016}
}

@article{Aeberhard2018,
abstract = {Fisheries science is concerned with the management and understanding of the raising and harvesting of fish. Fish stocks are assessed using biological and fisheries data with the goal of estimating either their total population or biomass. Stock assessment models also make it possible to predict how stocks will respond to varying levels of fishing pressure in the future. Such tools are essential with overfishing now reducing stocks and employment worldwide, with in turn many serious social, economic, and environmental implications. Increasingly, a state-space framework is being used in place of deterministic and standard parametric stock assessment models. These efforts have not only had considerable impact on fisheries management but have also advanced the supporting statistical theory and inference tools as well as the required software. An application of such techniques to the North Sea cod stock highlights what should be considered best practices for science-based fisheries management.},
author = {Aeberhard, William H and Flemming, Joanna Mills and Nielsen, Anders},
doi = {10.1146/annurev-statistics},
file = {:C$\backslash$:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aeberhard, Flemming, Nielsen - 2018 - Annual Review of Statistics and Its Application Review of State-Space Models for Fisheries Science.pdf:pdf},
journal = {Annual Review of Statistics and Its Application},
keywords = {fish stock assessment,population dynamics,random effects prediction,state-space assessment model},
mendeley-groups = {LA implementations/applications},
pages = {215--235},
title = {{Review of State-Space Models for Fisheries Science}},
url = {https://doi.org/10.1146/annurev-statistics-},
volume = {5},
year = {2018}
}

@article{Kristensen2016,
abstract = {TMB is an open source R package that enables quick implementation of complex nonlinear random effects (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, http://admb-project.org/; Fournier et al. 2011). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects (≈ 106) and parameters (≈ 103). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org/.},
archivePrefix = {arXiv},
arxivId = {1509.00660},
author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Bradley M.},
doi = {10.18637/jss.v070.i05},
eprint = {1509.00660},
file = {:C$\backslash$:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kristensen et al. - 2016 - TMB Automatic differentiation and laplace approximation.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {AD,Automatic differentiation,C++ templates,Latent variables,R,Random effects},
mendeley-groups = {LA implementations/applications},
month = {apr},
number = {1},
pages = {1--21},
publisher = {American Statistical Association},
title = {{TMB: Automatic differentiation and laplace approximation}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v070i05/v70i05.pdf https://www.jstatsoft.org/index.php/jss/article/view/v070i05},
volume = {70},
year = {2016}
}

  @Article{Nielsen2014,
    title = {Estimation of time-varying selectivity in stock assessments using state-space models},
    author = {Anders Nielsen and Casper W. Berg},
    journal = {Fisheries Research},
    year = {2014},
    volume = {158},
    pages = {96--101},
    doi = {10.1016/j.fishres.2014.01.014},
  }

  @article{Berg2016,
    title = {Accounting for correlated observations in an age-based state-space stock assessment model},
    author = {Casper W. Berg and Anders Nielsen},
    journal = {ICES Journal of Marine Science: Journal du Conseil},
    year = {2016},
    volume = {73},
    pages = {1788--1797},
    doi = {10.1093/icesjms/fsw046},
  }

@article{JanKoopman2009,
abstract = {Importance sampling is used in many areas of modern econometrics to approximate unsolvable integrals. Its reliable use requires the sampler to possess a variance, for this guarantees a square root speed of convergence and asymptotic normality of the estimator of the integral. However, this assumption is seldom checked. In this paper we use extreme value theory to empirically assess the appropriateness of this assumption. Our main application is the stochastic volatility model, where importance sampling is commonly used for maximum likelihood estimation of the parameters of the model.},
author = {Koopman, Siem Jan and Shephard, Neil and Creal, Drew},
doi = {10.1016/j.jeconom.2008.10.002},
file = {:C$\backslash$:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jan Koopman, Shephard, Creal - 2009 - Testing the assumptions behind importance sampling.pdf:pdf},
journal = {Journal of Econometrics},
keywords = {Extreme value theory,Importance sampling,Simulation,Stochastic volatility},
mendeley-groups = {Importance sampling},
pages = {2--11},
title = {{Testing the assumptions behind importance sampling}},
url = {www.elsevier.com/locate/jeconom},
volume = {149},
year = {2009}
}

@techreport{Betancourt2018,
abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
archivePrefix = {arXiv},
arxivId = {1701.02434v2},
author = {Betancourt, Michael},
eprint = {1701.02434v2},
file = {:C$\backslash$:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Carlo(2).pdf:pdf},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}},
year = {2018}
}

@techreport{Chai2019,
abstract = {We present an improved Bayesian framework for performing inference of affine transformations of constrained functions. We focus on quadrature with nonnegative functions, a common task in Bayesian inference. We consider constraints on the range of the function of interest , such as nonnegativity or boundedness. Although our framework is general, we derive explicit approximation schemes for these constraints , and argue for the use of a log transformation for functions with high dynamic range such as likelihood surfaces. We propose a novel method for optimizing hyperparameters in this framework: we optimize the marginal likelihood in the original space, as opposed to in the transformed space. The result is a model that better explains the actual data. Experiments on synthetic and real-world data demonstrate our framework achieves superior estimates using less wall-clock time than existing Bayesian quadrature procedures.},
author = {Chai, Henry and Garnett, Roman},
file = {:C$\backslash$:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chai, Garnett - 2019 - Improving Quadrature for Constrained Integrands.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
title = {{Improving Quadrature for Constrained Integrands}},
year = {2019}
}

@book{Chopin2020,
address = {Cham},
author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
doi = {10.1007/978-3-030-47845-2},
isbn = {978-3-030-47844-5},
mendeley-groups = {Other SSM stuff},
publisher = {Springer International Publishing},
series = {Springer Series in Statistics},
title = {{An Introduction to Sequential Monte Carlo}},
url = {https://link.springer.com/10.1007/978-3-030-47845-2},
year = {2020}
}

@article{Murray2015,
abstract = {LibBi is a software package for state space modelling and Bayesian inference on modern computer hardware, including multi-core central processing units, many-core graphics processing units, and distributed-memory clusters of such devices. The software parses a domain-specific language for model specification, then optimizes, generates, compiles and runs code for the given model, inference method and hardware platform. In presenting the software, this work serves as an introduction to state space models and the specialized methods developed for Bayesian inference with them. The focus is on sequential Monte Carlo (SMC) methods such as the particle filter for state estimation, and the particle Markov chain Monte Carlo and SMC2 methods for parameter estimation. All are well-suited to current computer hardware. Two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear Lorenz '96 model. These are specified in the prescribed modelling language, and LibBi demonstrated by performing inference with them. Empirical results are presented, including a performance comparison of the software with different hardware configurations.},
archivePrefix = {arXiv},
arxivId = {1306.3277},
author = {Murray, Lawrence M.},
doi = {10.18637/jss.v067.i10},
eprint = {1306.3277},
file = {::},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Bayesian hierarchical modelling,LibBi,Particle markov chain monte carlo,Sequential monte carlo,State space modelling},
mendeley-groups = {Other SSM stuff},
month = {oct},
number = {10},
publisher = {American Statistical Association},
title = {{Bayesian state-space modelling on high-performance hardware using LibBi}},
volume = {67},
year = {2015}
}

@article{Skaug2006,
abstract = {Fitting of non-Gaussian hierarchical random effects models by approximate maximum likelihood can be made automatic to the same extent that Bayesian model fitting can be automated by the program BUGS. The word "automatic" means that the technical details of computation are made transparent to the user. This is achieved by combining a technique from computer science known as "automatic differentiation" with the Laplace approximation for calculating the marginal likelihood. Automatic differentiation, which should not be confused with symbolic differentiation, is mostly unknown to statisticians, and hence basic ideas and results are reviewed. The computational performance of the approach is compared to that of existing mixed-model software on a suite of datasets selected from the mixed-model literature.},
author = {Skaug, Hans J and Fournier, David A},
doi = {10.1016/j.csda.2006.03.005},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skaug, Fournier - 2006 - Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
keywords = {AD Model Builder,Automatic differentiation,Importance sampling,Laplace approximation,Mixed models,Random effects},
mendeley-groups = {LA implementations/applications},
pages = {699--709},
title = {{Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models}},
url = {www.elsevier.com/locate/csda},
volume = {51},
year = {2006}
}

@article{Koyama2010,
abstract = {State-space models provide an important body of techniques for analyzing time series, but their use requires estimating unobserved states. The optimal estimate of the state is its conditional expectation given the observation histories, and computing this expectation is hard when there are nonlinearities. Existing filtering methods, including sequential Monte Carlo, tend to be either inaccurate or slow. In this paper, we study a nonlinear filter for nonlinear/non-Gaussian state-space models, which uses Laplace's method, an asymptotic series expansion, to approximate the state's conditional mean and variance, together with a Gaussian conditional distribution. This Laplace Gaussian filter (LGF) gives fast, recursive, deterministic state estimates, with an error which is set by the stochastic characteristics of the model and is, we show, stable over time. We illustrate the estimation ability of the LGF by applying it to the problem of neural decoding and compare it to sequential Monte Carlo both in simulations and with real data. We find that the LGF can deliver superior results in a small fraction of the computing time. This article has supplementary material online.},
author = {Koyama, Shinsuke and {Castellanos P{\'{e}}rez-bolde}, Lucia and Shalizi, Cosma Rohilla and Kass, Robert E.},
doi = {10.1198/jasa.2009.tm08326},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koyama et al. - 2010 - Approximate Methods for State-Space Models.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Laplace's method,Neural decoding,Recursive Bayesian estimation},
mendeley-groups = {LA implementations/applications,Other SSM stuff},
number = {489},
pages = {170--180},
title = {{Approximate Methods for State-Space Models}},
volume = {105},
year = {2010}
}

@article{Hennig2015,
abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return un...},
author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
doi = {10.1098/rspa.2015.0142},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hennig, Osborne, Girolami - 2015 - Probabilistic numerics and uncertainty in computations.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {inference,numerical methods,probability,statistics},
mendeley-groups = {Bayesian Quadrature},
month = {jul},
number = {2179},
pages = {20150142},
publisher = {
The Royal Society Publishing
},
title = {{Probabilistic numerics and uncertainty in computations}},
url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2015.0142},
volume = {471},
year = {2015}
}

@book{barndorff1989asymptotic,
author = {Barndorff-Nielsen, O E and Cox, D R and Cox, H.F.D.R.},
isbn = {9780412314001},
publisher = {Springer US},
series = {Asymptotic Techniques for Use in Statistics},
title = {{Asymptotic Techniques for Use in Statistics}},
url = {https://books.google.ca/books?id=UQ9yIrZpMToC},
year = {1989}
}

@book{de1981asymptotic,
author = {{De Bruijn}, Nicolaas Govert},
publisher = {Courier Corporation},
title = {{Asymptotic methods in analysis}},
volume = {4},
year = {1981}
}

@inproceedings{Lindley1961,
author = {Lindley, D. V.},
booktitle = {Proc. 4th Berkeley Symp. on Math. Stat. and Prob.},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindley - 1961 - The Use of Prior Probability Distributions in Statistical Inference and Decisions.pdf:pdf},
month = {jan},
number = {1},
pages = {453--468},
publisher = {University of California Press},
title = {{The Use of Prior Probability Distributions in Statistical Inference and Decisions}},
volume = {4},
year = {1961}
}

@article{Tierney1986,
abstract = {REFERENCES Linked references are available on JSTOR for this article: https://www.jstor.org/stable/2287970?seq=1&cid=pdf-reference#references_tab_contents You may need to log in to JSTOR to access the linked references. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
author = {Tierney, Luke and Kadane, Joseph B.},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kadane - 1986 - Accurate Approximations for Posterior Moments and Marginal Densities.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Asymp-totic expansions,Bayesian inference,Computation of integrals,Laplace method},
number = {393},
pages = {82--86},
title = {{Accurate Approximations for Posterior Moments and Marginal Densities}},
volume = {81},
year = {1986}
}

@article{Briol2019,
abstract = {A research frontier has emerged in scientific computation, wherein discretisation error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow, in order to assess the impact of discretisation error on the computer output. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the fact that the integrand has been discretised. Our main technical contribution is to establish, for the first time, rates of posterior contraction for one such method. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
archivePrefix = {arXiv},
arxivId = {1512.00933v6},
author = {Briol, Fran{\c{c}}ois-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
eprint = {1512.00933v6},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Briol et al. - 2017 - Probabilistic Integration A Role in Statistical Computation.pdf:pdf},
journal = {Statistical Science},
mendeley-groups = {Bayesian Quadrature},
number = {1},
pages = {1--22},
title = {{Probabilistic Integration: A Role in Statistical Computation?}},
url = {http://www.},
volume = {34},
year = {2019}
}

@article{Cockayne2019,
abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
archivePrefix = {arXiv},
arxivId = {1702.03673},
author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
eprint = {1702.03673},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cockayne et al. - 2017 - Bayesian Probabilistic Numerical Methods.pdf:pdf},
journal = {SIAM Review},
mendeley-groups = {Bayesian Quadrature},
month = {feb},
number = {4},
pages = {756--789},
title = {{Bayesian Probabilistic Numerical Methods}},
url = {http://arxiv.org/abs/1702.03673},
volume = {61},
year = {2019}
}

@article{Karvonen2018,
abstract = {Kernel quadratures and other kernel-based approximation methods typically suffer from prohibitive cubic time and quadratic space complexity in the number of function evaluations. The problem arises because a system of linear equations needs to be solved. In this article we show that the weights of a kernel quadrature rule can be computed efficiently and exactly for up to tens of millions of nodes if the kernel, integration domain, and measure are fully symmetric and the node set is a union of fully symmetric sets. This is based on the observations that in such a setting there are only as many distinct weights as there are fully symmetric sets and that these weights can be solved from a linear system of equations constructed out of row sums of certain submatrices of the full kernel matrix. We present several numerical examples that show feasibility, both for a large number of nodes and in high dimensions, of the developed fully symmetric kernel quadrature rules. Most prominent of the fully symmetric kernel quadrature rules we propose are those that use sparse grids.},
archivePrefix = {arXiv},
arxivId = {1703.06359},
author = {Karvonen, Toni and S{\"{a}}rkk{\"{a}}, Simo},
doi = {10.1137/17M1121779},
eprint = {1703.06359},
file = {:C\:/Users/Shaun/Downloads/17m1121779.pdf:pdf},
issn = {10957197},
journal = {SIAM Journal on Scientific Computing},
keywords = {Bayesian quadrature,Fully symmetric sets,Kernel quadrature,Numerical integration,Reproducing kernel Hilbert spaces,Sparse grids},
mendeley-groups = {Bayesian Quadrature},
month = {mar},
number = {2},
pages = {A697--A720},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Fully symmetric kernel quadrature}},
volume = {40},
year = {2018}
}

@article{OHagan1991,
abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in IRP. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes-Hermite quadrature rules may perform better than the conventional Gauss-Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
author = {O'Hagan, A.},
doi = {10.1016/0378-3758(91)90002-V},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan - 1991 - Bayes-Hermite quadrature.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
keywords = {statistics},
mendeley-groups = {Bayesian Quadrature},
pages = {245--260},
title = {{Bayes-Hermite quadrature}},
volume = {29},
year = {1991}
}

@phdthesis{Osborne2010,
abstract = {We develop a family of Bayesian algorithms built around Gaussian processes for various problems posed by sensor networks. We firstly introduce an iterative Gaussian process for multi-sensor inference problems, and show how our algorithm is able to cope with data that may be noisy, missing, delayed and/or correlated. Our algorithm can also effectively manage data that features changepoints, such as sensor faults. Extensions to our algorithm allow us to tackle some of the decision problems faced in sensor networks, including observation scheduling. Along these lines, we also propose a general method of global optimisation, Gaussian process global optimisation (GPGO), and demonstrate how it may be used for sensor placement. Our algorithms operate within a complete Bayesian probabilistic framework. As such, we show how the hyperparameters of our system can be marginalised by use of Bayesian quadrature, a principled method of approximate integration. Similar techniques also allow us to produce full posterior distributions for any hyperparameters of interest, such as the location of changepoints. We frame the selection of the positions of the hyperparameter samples required by Bayesian quadrature as a decision problem , with the aim of minimising the uncertainty we possess about the values of the integrals we are approximating. Taking this approach, we have developed sampling for Bayesian quadrature (SBQ), a principled competitor to Monte Carlo methods. We conclude by testing our proposals on real weather sensor networks. We further benchmark GPGO on a wide range of canonical test problems, over which it achieves a significant improvement on its competitors. Finally, the efficacy of SBQ is demonstrated in the context of both prediction and optimisation.},
author = {Osborne, Michael},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osborne - 2010 - Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature.pdf:pdf},
keywords = {()},
mendeley-groups = {Bayesian Quadrature},
school = {University of Oxford},
title = {{Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature}},
year = {2010}
}

@book{Rasmussen2006,
abstract = {Regression -- Classification -- Covariance functions -- Model selection and adaptation of hyperparameters -- Relationships between GPs and other models -- Theoretical perspectives -- Approximation methods for large datasets -- Appendix A : Mathematical background -- Appendix B : Guassian Markov processes.},
author = {Rasmussen, Carl Edward. and Williams, Christopher K. I.},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams - 2006 - Gaussian processes for machine learning.pdf:pdf},
isbn = {9780262182539},
mendeley-groups = {GP stuff},
pages = {248},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
year = {2006}
}


@inproceedings{Huszar2012,
  title={Optimally-weighted herding is Bayesian quadrature},
  author={Husz{\'a}r, Ferenc and Duvenaud, David},
  booktitle={Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages={377--386},
  year={2012}
}

@techreport{Minka2000,
author = {Minka, Thomas P.},
file = {:C\:/Users/Shaun/Downloads/minka-quadrature.pdf:pdf},
institution = {Carnegie Mellon University},
mendeley-groups = {GP stuff,Bayesian Quadrature},
pages = {1--21},
title = {{Deriving quadrature rules from Gaussian processes}},
year = {2000}
}

@inproceedings{Gunter2014,
abstract = {We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in proba-bilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (like-lihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.},
author = {Gunter, Tom and Osborne, Michael A and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J},
booktitle = {Advances in Neural Information Processing Systems},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gunter et al. - Unknown - Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
pages = {2789--2797},
title = {{Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature}},
year = {2014}
}

@inproceedings{Osborne2012,
abstract = {Numerical integration is a key component of many problems in scientific computing , statistical modelling, and machine learning. Bayesian Quadrature is a model-based method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normal-ising constant of a probabilistic model. Our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.},
author = {Osborne, Michael A and Duvenaud, David and Garnett, Roman and Rasmussen, Carl E and Roberts, Stephen J and Ghahramani, Zoubin},
booktitle = {Advances in neural information processing systems},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osborne et al. - Unknown - Active Learning of Model Evidence Using Bayesian Quadrature.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
pages = {46--54},
title = {{Active Learning of Model Evidence Using Bayesian Quadrature}},
year = {2012}
}

@mastersthesis{Kaarnioja2013,
author = {Kaarnioja, Vesa},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaarnioja - Unknown - Smolyak Quadrature.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
pages = {1--75},
school = {University of Helsinki},
title = {{Smolyak Quadrature}},
year = {2013}
}

@book{Jolliffe2002,
address = {New York},
author = {Jolliffe, I.T.},
doi = {10.1007/b98835},
edition = {2},
file = {:C\:/Users/Shaun/Documents/Jolliffe I. Principal Component Analysis (2ed., Springer, 2002)(518s)_MVsa_.pdf:pdf},
isbn = {0-387-95442-2},
publisher = {Springer-Verlag New York},
series = {Springer Series in Statistics},
title = {{Principal Component Analysis}},
url = {http://link.springer.com/10.1007/b98835},
year = {2002}
}

@inproceedings{Abe2005,
abstract = {Radial basis function (RBF) kernels are widely used for support vector machines. But for model selection, we need to optimize the kernel parameter and the margin parameter by time-consuming cross validation. To solve this problem, in this paper we propose using Mahalanobis kernels, which are generalized HBF kernels. We determine the covariance matrix for the Mahalanobis kernel using the training data corresponding to the associated classes. Model selection is done by line search. Namely, first the margin parameter is optimized and then the Mahalanobis kernel parameter is optimized. According to the computer experiments for two-class problems, a Mahalanobis kernel with a diagonal covariance matrix shows better generalization ability than a Mahalanobis kernel with a full covariance matrix, and a Mahalanobis kernel optimized by line search shows comparable performance with that with an RBF kernel optimized by grid search. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Abe, Shigeo},
booktitle = {Proc. International Conference on Artificial Neural Networks (ICANN 2005)},
doi = {10.1007/11550907_90},
file = {::},
isbn = {3540287558},
issn = {03029743},
mendeley-groups = {GP stuff},
pages = {571--576},
publisher = {Springer, Berlin, Heidelberg},
title = {{Training of support vector machines with Mahalanobis kernels}},
url = {http://www2.eedept.kobe-u.ac.jp/$\sim$abe},
year = {2005}
}

@techreport{Sarkka2015,
abstract = {This article is concerned with Gaussian process quadratures, which are numerical integration methods based on Gaussian process regression methods, and sigma-point methods, which are used in advanced non-linear Kalman filtering and smoothing algorithms. We show that many sigma-point methods can be interpreted as Gaussian quadrature based methods with suitably selected covariance functions. We show that this interpretation also extends to more general multivariate Gauss-Hermite integration methods and related spherical cubature rules. Additionally, we discuss different criteria for selecting the sigma-point locations: exactness for multivariate polynomials up to a given order, minimum average error, and quasi-random point sets. The performance of the different methods is tested in numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1504.05994v1},
author = {S{\"{a}}rkk{\"{a}}, Simo and Hartikainen, Jouni and Svensson, Lennart and Sandblom, Fredrik},
eprint = {1504.05994v1},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\"{a}}rkk{\"{a}} et al. - Unknown - On the relation between Gaussian process quadratures and sigma-point methods.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
pages = {1--13},
title = {{On the relation between Gaussian process quadratures and sigma-point methods}},
year = {2015}
}

@inproceedings{Rasmussen2003,
abstract = {We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evi-dences). We find that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.},
author = {Rasmussen, Carl Edward and Ghahramani, Zoubin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Ghahramani - Unknown - Bayesian Monte Carlo.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
pages = {489--496},
title = {{Bayesian Monte Carlo}},
url = {http://www.gatsby.ucl.ac.uk},
year = {2003}
}

@article{Kennedy1998,
abstract = {We consider an ecient Bayesian approach to estimating integration-based posterior summaries from a separate Bayesian application. In Bayesian quadrature we model an intractable posterior density function f {\'{A}} as a Gaussian process, using an approximating function g{\'{A}}, and {\textregistered}nd a posterior distribution for the integral of f {\'{A}}, conditional on a few evaluations of f {\'{A}} at selected design points. Bayesian quadrature using normal g{\'{A}} is called Bayes-Hermite quadrature. We extend this theory by allowing g{\'{A}} to be chosen from two wider classes of functions. One is a family of skew densities and the other is the family of {\textregistered}nite mixtures of normal densities. For the family of skew densities we describe an iterative updating procedure to select the most suitable approximation and apply the method to two simulated posterior density functions.},
author = {Kennedy, Marc},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy - 1998 - Bayesian quadrature with non-normal approximating functions.pdf:pdf},
journal = {Statistics and Computing},
keywords = {Bayesian quadrature,Gaussian process,approximating posterior densities; {\textregistered}nite mixture o,numerical integration},
mendeley-groups = {Bayesian Quadrature},
pages = {365--375},
title = {{Bayesian quadrature with non-normal approximating functions}},
volume = {8},
year = {1998}
}

@article{Yuan2007,
abstract = {The AIS-BN algorithm [J. Cheng, M.J. Druzdzel, BN-AIS: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks, Journal of Artificial Intelligence Research 13 (2000) 155-188] is a successful importance sampling-based algorithm for Bayesian networks that relies on two heuristic methods to obtain an initial importance function:-cutoff, replacing small probabilities in the conditional probability tables by a larger , and setting the probability distributions of the parents of evidence nodes to uniform. However, why the simple heuristics are so effective was not well understood. In this paper, we point out that it is due to a practical requirement for the importance function, which says that a good importance function should possess thicker tails than the actual posterior probability distribution. By studying the basic assumptions behind importance sampling and the properties of importance sampling in Bayesian networks, we develop several theoretical insights into the desirability of thick tails for importance functions. These insights not only shed light on the success of the two heuristics of AIS-BN, but also provide a common theoretical basis for several other successful heuristic methods.},
author = {Yuan, Changhe and Druzdzel, Marek J.},
doi = {10.1016/j.ijar.2006.09.006},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Druzdzel - 2007 - Theoretical analysis and practical insights on importance sampling in Bayesian networks.pdf:pdf},
journal = {International Journal of Approximate Reasoning},
mendeley-groups = {Importance sampling},
pages = {320--333},
title = {{Theoretical analysis and practical insights on importance sampling in Bayesian networks}},
url = {www.elsevier.com/locate/ijar},
volume = {46},
year = {2007}
}

@article{Tokdar2009,
author = {Tokdar, Surya T. and Kass, Robert E},
doi = {10.1002/wics.56},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tokdar, Kass - 2009 - Importance sampling a review.pdf:pdf},
journal = {Advanced Review},
keywords = {Markov chain sampling,Monte Carlo approximation,importance sampling,resampling,sequential sampling},
mendeley-groups = {Importance sampling},
pages = {54--60},
publisher = {John Wiley & Sons, Inc. WIREs Comp Stat},
title = {{Importance sampling: a review}},
volume = {2},
year = {2009}
}

@inproceedings{Pruher2017,
abstract = {The aim of this article is to design a moment transformation for Student-t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student-t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.},
address = {Xi'an, China},
author = {Pr{\"{u}}her, Jakub and Tronarp, Filip and Karvonen, Toni and S{\"{a}}rkk{\"{a}}, Simo and Straka, Ondřej},
booktitle = {20th International Conference on Information Fusion},
doi = {10.23919/ICIF.2017.8009742},
file = {:C\:/Users/Shaun/Downloads/08009742.pdf:pdf},
mendeley-groups = {Bayesian Quadrature},
month = {aug},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Student-t process quadratures for filtering of non-linear systems with heavy-tailed noise}},
year = {2017}
}

@article{Haario1999,
abstract = {The choice of a suitable MCMC method and further the choice of a proposal distribution is known to be crucial for the convergence of the Markov chain. However, in many cases the choice of an effective proposal distribution is difficult. As a remedy we suggest a method called Adaptive Proposal (AP). Although the stationary distribution of the AP algorithm is slightly biased, it appears to provide an efficient tool for, e.g., reasonably low dimensional problems, as typically encountered in non-linear regression problems in natural sciences. As a realistic example we include a successful application of the AP algorithm in parameter estimation for the satellite instrument 'GOMOS'. In this paper we also present systematic performance criteria for comparing Adaptive Proposal algorithm with more traditional Metropolis algorithms.},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario, Saksman, Tamminen - 1999 - Adaptive proposal distribution for random walk Metropolis algorithm.pdf:pdf},
journal = {Computational Statistics},
keywords = {Adaptive MCMC,MCMC,Metropolis-Hastings algorithm,convergence 376},
pages = {375--395},
title = {{Adaptive proposal distribution for random walk Metropolis algorithm}},
volume = {14},
year = {1999}
}

@manual{MatlabOTB,
  title = {MATLAB Optimization Toolbox},
  year = {2019},
  author = {The {MathWorks, Inc.}}, 
  address = {Natick, MA, USA},
  url = {https://www.mathworks.com/help/pdf_doc/optim/optim.pdf}
}

@misc{carpenter2017,
  title={Typical sets and the curse of dimensionality},
  author={Carpenter, Bob},
  journal={Stan Software},
  year={2017},
url = {https://mc-stan.org/users/documentation/case-studies/curse-dims.html}
}

@techreport{Julier1996,
abstract = {In this paper we describe a new approach for generalised nonlinear ltering. We show that the technique is more accurate, more stable, and far easier to implement than an extended Kalman lter. Several examples are provided, including the application of the new lter to problems involving discontinuous functions.},
address = {Oxford},
author = {Julier, Simon and Uhlmann, Jeffrey K.},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Julier, Uhlmann - 1996 - A General Method for Approximating Nonlinear Transformations of Probability Distributions.pdf:pdf},
institution = {University of Oxford},
mendeley-groups = {Kalman Filter},
title = {{A General Method for Approximating Nonlinear Transformations of Probability Distributions}},
year = {1996}
}

@inproceedings{Ienkaran2009,
abstract = {In this paper, we present a new nonlinear filter for high-dimensional state estimation, which we have named the cubature Kalman filter (CKF). The heart of the CKF is a spherical-radial cubature rule, which makes it possible to numerically compute multivariate moment integrals encountered in the nonlinear Bayesian filter. Specifically, we derive a third-degree spherical-radial cubature rule that provides a set of cubature points scaling linearly with the state-vector dimension. The CKF may therefore provide a systematic solution for high-dimensional nonlinear filtering problems. The paper also includes the derivation of a square-root version of the CKF for improved numerical stability. The CKF is tested experimentally in two nonlinear state estimation problems. In the first problem, the proposed cubature rule is used to compute the second-order statistics of a nonlinearly transformed Gaussian random variable. The second problem addresses the use of the CKF for tracking a maneuvering aircraft. The results of both experiments demonstrate the improved performance of the CKF over conventional nonlinear filters. {\textcopyright} 2009 IEEE.},
author = {Ienkaran and Haykin, Simon},
booktitle = {IEEE Transactions on Automatic Control},
doi = {10.1109/TAC.2009.2019800},
file = {:C\:/Users/Shaun/Downloads/04982682.pdf:pdf},
keywords = {Bayesian filters,Cubature rules,Gaussian quadrature rules,Invariant theory,Kalman filter,Nonlinear filtering},
mendeley-groups = {Kalman Filter},
number = {6},
pages = {1254--1269},
title = {{Cubature kalman filters}},
volume = {54},
year = {2009}
}

@book{Blum2020,
abstract = {This book provides an introduction to the mathematical and algorithmic foundations of data science, including machine learning, high-dimensional geometry, and analysis of large networks. Topics include the counterintuitive nature of data in high dimensions, important linear algebraic techniques such as singular value decomposition, the theory of random walks and Markov chains, the fundamentals of and important algorithms for machine learning, algorithms and analysis for clustering, probabilistic models for large networks, representation learning including topic modelling and non-negative matrix factorization, wavelets and compressed sensing. Important probabilistic techniques are developed including the law of large numbers, tail inequalities, analysis of random projections, generalization guarantees in machine learning, and moment methods for analysis of phase transitions in large random graphs. Additionally, important structural and complexity measures are discussed such as matrix norms and VC-dimension. This book is suitable for both undergraduate and graduate courses in the design and analysis of algorithms for data.},
author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
booktitle = {Foundations of Data Science},
doi = {10.1017/9781108755528},
file = {::},
isbn = {9781108755528},
month = {jan},
publisher = {Cambridge University Press},
title = {{Foundations of Data Science}},
url = {https://www-cambridge-org.proxy.lib.sfu.ca/core/books/foundations-of-data-science/6A43CE830DE83BED6CC5171E62B0AA9E},
year = {2020}
}

@article{Geweke1989,
author = {Geweke, John},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geweke - 1989 - Bayesian Inference in Econometric Models Using Monte Carlo Integration.pdf:pdf},
journal = {Econometrica},
mendeley-groups = {Importance sampling},
number = {6},
pages = {1317--1339},
title = {{Bayesian Inference in Econometric Models Using Monte Carlo Integration}},
volume = {57},
year = {1989}
}

@inproceedings{Rao1948,
  title={Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation},
  author={Rao, C Radhakrishna},
  booktitle={Mathematical Proceedings of the Cambridge Philosophical Society},
  volume={44},
  number={1},
  pages={50--57},
  year={1948},
  organization={Cambridge University Press}
}

@article{Evans1995,
author = {Evans, Michael and Swartz, Tim},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Swartz - 1995 - Methods for Approximating Integrals in Statistics with Special Emphasis on Bayesian Integration Problems.pdf:pdf},
journal = {Statistical Science},
mendeley-groups = {Importance sampling},
number = {3},
pages = {254--272},
title = {{Methods for Approximating Integrals in Statistics with Special Emphasis on Bayesian Integration Problems}},
volume = {10},
year = {1995}
}
@article{DeValpine2002,
abstract = {A key challenge for analyzing fisheries time-series data has been to incorporate sources of uncertainty such as process error, observation error, and model-structure uncertainty. Recent years have seen promising advances in methods for handling the first two together in a state-space framework, but likelihood calculations for state-space models require high-dimensional integrals, which make their use computationally challenging. The first section of this paper reviews model-fitting methods that use a state-space model structure, including errors-in-variables methods, Bayesian methods that do and do not use the state-space likelihood, and the possibility of classical likelihood analysis with nonlinear, non-Gaussian state-space models. It also discusses the relationship between true likelihood calculations and errors-in-variables likelihoods, as well as the role of Monte Carlo methods in implementing Bayesian and/or state-space model analyses. The second section introduces a numerical method for calculating state-space likelihoods without Monte Carlo methods and gives examples in a classical maximum-likelihood framework. The method is applicable when the dimension of the state space at each time step is low. Although recent advances in model-fitting and analysis methods are promising , inferences from noisy data and complex processes will continue to be variable and uncertain. Some types of inference about fisheries dynamics rely on fitting time-series data to mathematical models of population dynamics and harvesting. Not long ago, a wide dis-junction separated roughly realistic models and statistically usable models; one could not take just any model and estimate parameters and their uncertainty without taking more or less awkward steps. One reason was that any realistic model must include variability in how populations change-process error-as well as variability in estimates of the true state of the population-observation error. Together, these two greatly complicate the statistical relationship between model and data. Recent years have seen wholesale improvements in methods for incorporating both process and observation error into statistical analysis of fisheries data, but these methods have also brought to the fore several complex issues. The simplest method for handling process and observation error is to assume that one or the other is absent (see, e.g., Hilborn, 1979; Polacheck et al., 1993; Raftery et al., 1995; Kinas, 1996). Later methods include both but require the assumption that a ratio involving their variances is known (Ludwig and Walters, 1981, 1989; Walters and Ludwig, 1981; Collie and Sissenwine, 1983; Ludwig et al., 1988; Schnute, 1994; Schnute and Richards, 1995; Richards et al., 1997; Fargo and Richards, 1998). What these methods have in common is that the fit of model to data is measured by only a single estimated population trajectory that might have produced the data. The latest methods incorporate the full range of population trajectories that might have produced the data to calculate the fit of model to data, and they do not require assumptions about relative magnitudes of},
author = {de Valpine, Perry},
file = {:C\:/Users/Shaun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Valpine - 2002 - Review of methods for fitting time-series models with process and observation error and likelihood calculations for.pdf:pdf},
journal = {Bulletin of Marine Science},
mendeley-groups = {Other SSM stuff},
number = {2},
pages = {455--471},
title = {{Review of methods for fitting time-series models with process and observation error and likelihood calculations for nonlinear, non-Gaussian state-space models}},
volume = {70},
year = {2002}
}





